{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation.txt              \u001b[0m\u001b[01;35moverlap.txt2frames.gif\u001b[0m\r\n",
      "\u001b[01;35mAggregation.txt12frames.gif\u001b[0m  \u001b[01;35moverlap.txt4frames.gif\u001b[0m\r\n",
      "\u001b[01;35mAggregation.txt20frames.gif\u001b[0m  \u001b[01;35moverlap.txt5frames.gif\u001b[0m\r\n",
      "\u001b[01;35mAggregation.txt22frames.gif\u001b[0m  \u001b[01;35moverlap.txt6frames.gif\u001b[0m\r\n",
      "\u001b[01;35mAggregation.txt3frames.gif\u001b[0m   \u001b[01;35moverlap.txt7frames.gif\u001b[0m\r\n",
      "\u001b[01;35manimation-400.mp4\u001b[0m            pogo.ipynb\r\n",
      "\u001b[01;35manimation-b.mp4\u001b[0m              R15.txt\r\n",
      "\u001b[01;35manimation-c.mp4\u001b[0m              \u001b[01;35mR15.txt22frames.gif\u001b[0m\r\n",
      "\u001b[01;35manimation-d.mp4\u001b[0m              \u001b[01;35mR15.txt40frames.gif\u001b[0m\r\n",
      "\u001b[01;35manimation.gif\u001b[0m                \u001b[01;35mR15.txt50frames.gif\u001b[0m\r\n",
      "\u001b[01;35manimation.mp4\u001b[0m                \u001b[01;35mR15.txt60frames.gif\u001b[0m\r\n",
      "Compound.txt                 \u001b[01;35mR15.txt80frames.gif\u001b[0m\r\n",
      "\u001b[01;35mCompound.txt22frames.gif\u001b[0m     \u001b[01;35mR15.txt93frames.gif\u001b[0m\r\n",
      "D31.txt                      README.md\r\n",
      "\u001b[01;35mD31.txt19frames.gif\u001b[0m          scikitlearn-clustering-data.ipynb\r\n",
      "\u001b[01;35mD31.txt22frames.gif\u001b[0m          \u001b[01;35mspiral20frames.mp4\u001b[0m\r\n",
      "\u001b[01;35mjain40frames.mp4\u001b[0m             \u001b[01;35mspiral40frames.mp4\u001b[0m\r\n",
      "jain.txt                     spiral.txt\r\n",
      "\u001b[01;35mjain.txt20frames.gif\u001b[0m         \u001b[01;35mspiral.txt20frames.gif\u001b[0m\r\n",
      "\u001b[01;35moverlap.txt19frames.gif\u001b[0m      \u001b[01;35mspiral.txt5frames.gif\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gudhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import animation\n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#randomized blobs with standard deviation\n",
    "n = 6\n",
    "std = np.random.random_sample((n,))\n",
    "#std = 1\n",
    "X, y, z = make_blobs(\n",
    "    n_samples=n*100,\n",
    "    n_features=2,\n",
    "    centers=n,\n",
    "    cluster_std=std,\n",
    "    center_box=(-10.0, 10.0),\n",
    "    shuffle=False,\n",
    "    random_state=42,\n",
    "    return_centers=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "size=8\n",
    "plt.figure(figsize=(size,size))\n",
    "plt.scatter(X[:, 0], X[:, 1],\n",
    "            s=25, \n",
    "            c=y,\n",
    "            marker=\"o\",\n",
    "            cmap='tab10',\n",
    "            norm=None,\n",
    "            alpha=.7,\n",
    "            edgecolor=\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data from scikitlearn tutorial on clustering \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), np.ones(n_samples,dtype='int')\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=varied[0]\n",
    "y=varied[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "urls_with_ground_truth = ['https://cs.joensuu.fi/sipu/datasets/D31.txt',\n",
    "        'https://cs.joensuu.fi/sipu/datasets/spiral.txt',\n",
    "        'https://cs.joensuu.fi/sipu/datasets/pathbased.txt',\n",
    "        'https://cs.joensuu.fi/sipu/datasets/R15.txt']\n",
    "\n",
    "urls_without_ground_truth = ['https://cs.joensuu.fi/sipu/datasets/D31.txt',\n",
    "        'https://cs.joensuu.fi/sipu/datasets/spiral.txt',\n",
    "        'https://cs.joensuu.fi/sipu/datasets/overlap.txt',\n",
    "        'https://cs.joensuu.fi/sipu/datasets/pathbased.txt',\n",
    "        'https://cs.joensuu.fi/sipu/datasets/a1.txt',\n",
    "        'https://cs.joensuu.fi/sipu/datasets/R15.txt']\n",
    "\n",
    "ground_truth_urls = ['https://cs.joensuu.fi/sipu/datasets/overlap.pa'\n",
    "    \n",
    "]\n",
    "\n",
    "urls_with_ground_truth = {\n",
    "    urlparse(url).path.strip('/').split('/')[-1] : url for url in urls_with_ground_truth\n",
    "}\n",
    "\n",
    "\n",
    "urls_without_ground_truth = {\n",
    "    urlparse(url).path.strip('/').split('/')[-1] : url for url in urls_without_ground_truth\n",
    "}\n",
    "\n",
    "\n",
    "ground_truth = {\n",
    "    urlparse(url).path.strip('/').split('/')[-1] : url for url in ground_truth_urls\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_with_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dataset(filename):\n",
    "    if filename in urls_with_ground_truth:\n",
    "        url= urls_with_ground_truth[filename]\n",
    "        print(url)\n",
    "        parse = urlparse(url)\n",
    "        print(filename)\n",
    "        raw_data = urlopen(url)\n",
    "        data = np.loadtxt(raw_data)\n",
    "\n",
    "        print(data.shape)\n",
    "        print(data[:10])\n",
    "\n",
    "        y = data[:,2]\n",
    "        print(y.shape)\n",
    "        X = data[:,:2]\n",
    "        print(X.shape)\n",
    "        return X, y\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = return_dataset('D31.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris = datasets.load_iris()\n",
    "#X = iris.data\n",
    "#y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=8\n",
    "plt.figure(figsize=(size,size))\n",
    "plt.scatter(X[:, 0], X[:, 1],\n",
    "            s=25, \n",
    "            c=y,\n",
    "            marker=\"o\",\n",
    "            cmap='tab10',\n",
    "            norm=None,\n",
    "            alpha=.7,\n",
    "            edgecolor=\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rips_complex = gudhi.RipsComplex(points=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplex_tree = rips_complex.create_simplex_tree(max_dimension=1)\n",
    "   \n",
    "print(simplex_tree.num_vertices() , 'vertices')  \n",
    "print(simplex_tree.num_simplices(), 'simplices')\n",
    "print('dimension:', simplex_tree.dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = simplex_tree.persistence()\n",
    "gudhi.plot_persistence_barcode(diag)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move through list and assign clusters to conected components\n",
    "point_dict={i:0 for i in range(simplex_tree.num_vertices())}\n",
    "counter=0\n",
    "cluster_dict_list = []\n",
    "distance_list = []\n",
    "for simplex in simplex_tree.get_filtration():\n",
    "    if len(simplex[0])>1:\n",
    "        if all(value > 0 for value in list(point_dict.values())):\n",
    "            if len(np.unique(np.array(list(point_dict.values())))) == 1:\n",
    "                print('break')\n",
    "                print(simplex)\n",
    "                print(simplex[1])\n",
    "                simplex_tree.prune_above_filtration(simplex[1])\n",
    "                break\n",
    "\n",
    "\n",
    "        #if both points are still in cluster 0, assign both to a new cluster\n",
    "        if point_dict[simplex[0][0]] == 0 and point_dict[simplex[0][1]] == 0:\n",
    "            counter += 1\n",
    "            point_dict[simplex[0][0]] = counter\n",
    "            point_dict[simplex[0][1]] = counter\n",
    "            #if one point is in cluster 0 and one is not, assign the one in cluster 0 to the existing cluster\n",
    "        elif point_dict[simplex[0][0]] == 0 and point_dict[simplex[0][1]] != 0:\n",
    "            point_dict[simplex[0][0]] = point_dict[simplex[0][1]]\n",
    "\n",
    "            #and vice versa\n",
    "        elif point_dict[simplex[0][0]] != 0 and point_dict[simplex[0][1]] == 0:\n",
    "            point_dict[simplex[0][1]] = point_dict[simplex[0][0]]\n",
    "\n",
    "            #if both points are not in cluster 0 and not in the same cluster, merge clusters to the lower number cluster\n",
    "        elif point_dict[simplex[0][0]] != 0 and point_dict[simplex[0][1]] != 0 and point_dict[simplex[0][0]] != point_dict[simplex[0][1]]:\n",
    "            larger_cluster_number = max(point_dict[simplex[0][0]], point_dict[simplex[0][1]])\n",
    "            smaller_cluster_number = min(point_dict[simplex[0][0]], point_dict[simplex[0][1]])\n",
    "            for key, value in point_dict.items():\n",
    "                if value == larger_cluster_number:\n",
    "                    point_dict[key] = smaller_cluster_number\n",
    "                    \n",
    "        distance_list.append(simplex[1])\n",
    "        cluster_dict_list.append(point_dict.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(distance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(distance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_array = np.array(distance_list)\n",
    "print(len(distance_array))\n",
    "distance_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(cluster_dict_list)\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simplex_tree.num_simplices(), 'simplices')\n",
    "#?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that all clusters have merged\n",
    "len(np.unique(np.array(list(point_dict.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_dict_list[-2].values()) #penultimate clustering has 2 clusters\n",
    "print(cluster_dict_list[-1].values()) #last clustering finally merges to all one cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the gaps between birth/death pairs\n",
    "gaps = np.diff(distance_array)\n",
    "gaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a zero back to the beginning of the gaps\n",
    "gaps = np.concatenate([np.zeros(1),gaps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find normalized distance\n",
    "scaler = MinMaxScaler()\n",
    "normed_distance = scaler.fit_transform(distance_array.reshape(-1,1)).T.reshape(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then invert them and multiply by the gaps to weight early connections in the filtration\n",
    "inverted_normed_distance = 1 - normed_distance\n",
    "inverted_normed_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_gaps = np.multiply(gaps, inverted_normed_distance)\n",
    "normed_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize to create a probability vector\n",
    "gap_vector = normed_gaps / np.sum(normed_gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "np.sum(gap_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gap_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = 0\n",
    "for i in range(1,length-1):\n",
    "    if cluster_dict_list[marker] == cluster_dict_list[i]:\n",
    "\n",
    "        gap_vector[marker] += gap_vector[i]\n",
    "        gap_vector[i] = 0\n",
    "        #print(marker)\n",
    "\n",
    "\n",
    "    else:\n",
    "        marker = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(gap_vector))\n",
    "plt.plot(gap_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = np.flip(np.argsort(gap_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_vector[candidates[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = candidates[0]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(list(cluster_dict_list[idx].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gap_vector[idx])\n",
    "print('percent confidence')\n",
    "'{:.1%}'.format(gap_vector[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(cluster_dict_list[idx].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_idx_array = (np.unique(np.array(list(cluster_dict_list[idx].values()))))\n",
    "#print(cluster_idx_array)\n",
    "#print(np.count_nonzero(cluster_idx_array))\n",
    "\n",
    "number_of_clusters = np.count_nonzero(np.unique(np.array(list(cluster_dict_list[idx].values()))))\n",
    "print(number_of_clusters)\n",
    "print('your dataset appears to have', number_of_clusters,'clusters, not including any outliers')\n",
    "#return counts from np.unique? to filter out small clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = y\n",
    "\n",
    "print(metrics.adjusted_rand_score(true, pred))\n",
    "print(metrics.silhouette_score(X, pred, metric=\"sqeuclidean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=12\n",
    "plt.figure(figsize=(size,size))\n",
    "plt.scatter(X[:, 0], X[:, 1],\n",
    "            s=50, \n",
    "            c=np.array(list(cluster_dict_list[idx].values())),\n",
    "            marker=\"o\",\n",
    "            cmap='flag',\n",
    "            norm=None,\n",
    "            alpha=.9,\n",
    "            edgecolor=\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = candidates[:100]\n",
    "idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = [x for x in idx_list if x < idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx_array = np.asarray(idx_list)\n",
    "idx_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_list = []\n",
    "for i in idx_list:\n",
    "    \n",
    "    silhouette = metrics.silhouette_score(X, np.array(list(cluster_dict_list[i].values())), metric=\"euclidean\")\n",
    "    silhouette_list.append(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "silhouette_array = np.asarray(silhouette_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(idx_array,silhouette_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = silhouette_array.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx_array[max_idx]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = silhouette_scores.index(max(silhouette_scores[simplex_tree.num_vertices():idx]))\n",
    "#print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_score_list = []\n",
    "true = y\n",
    "for i in idx_list:\n",
    "    pred = np.array(list(cluster_dict_list[i].values()))\n",
    "    rand_score = metrics.adjusted_rand_score( true, pred)\n",
    "    rand_score_list.append(rand_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_score_array = np.asarray(rand_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.plot(idx_array,rand_score_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(rand_score_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = idx_array[rand_score_array.argmax()]\n",
    "best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_max(array):\n",
    "    for i in range(len(array)):\n",
    "        if idx_array[i] > simplex_tree.num_vertices():\n",
    "\n",
    "            if array[i] > array[i+1]:\n",
    "                return float(array[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_max = get_first_max(silhouette_array)\n",
    "idx_array[np.where(silhouette_array == first_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(idx_array[np.where(silhouette_array == first_max)])\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(list(cluster_dict_list[idx].values()))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_score = metrics.adjusted_rand_score( true, pred)\n",
    "rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = np.ma.masked_where(pred == 0,pred)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_idx_array = (np.unique(np.array(list(cluster_dict_list[idx].values()))))\n",
    "#print(cluster_idx_array)\n",
    "#print(np.count_nonzero(cluster_idx_array))\n",
    "\n",
    "number_of_clusters = np.count_nonzero(np.unique(np.array(list(cluster_dict_list[idx].values()))))\n",
    "print(number_of_clusters)\n",
    "print('your dataset appears to have', number_of_clusters,'clusters, not including any outliers')\n",
    "#return counts from np.unique? to filter out small clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap(\"flag\").copy()\n",
    "cmap.set_bad(cmap(0))\n",
    "cmap.set_under(cmap(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap.get_bad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmap(number_of_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=12\n",
    "plt.figure(figsize=(size,size))\n",
    "plt.scatter(X[:, 0], X[:, 1],\n",
    "            s=50, \n",
    "            c=pred,\n",
    "            marker=\"o\",\n",
    "            cmap=cmap,\n",
    "            norm=None,\n",
    "            alpha=.9,\n",
    "            edgecolor=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "frames = len(idx_array)\n",
    "\n",
    "\n",
    "outfile = filename + str(frames) + 'frames.gif'\n",
    "\n",
    "if not os.path.isfile(outfile):\n",
    "    def init():\n",
    "        scatter = ax.scatter(X[:, 0], X[:, 1],\n",
    "                        s=40, \n",
    "                        c=np.array(list(cluster_dict_list[0].values())),\n",
    "                        marker=\"o\",\n",
    "                        cmap=cmap,\n",
    "                        norm=None,\n",
    "                        alpha=1,\n",
    "                        edgecolor=\"k\")\n",
    "        #ax.set(xlim=(-1, 35), ylim=(-1, 35))\n",
    "\n",
    "        return scatter,\n",
    "\n",
    "    #collection = PatchCollection(X, animated=True)\n",
    "\n",
    "    #ax.add_collection(collection)\n",
    "    #ax.autoscale_view(True)\n",
    "\n",
    "    def animate(i):\n",
    "\n",
    "        scatter = ax.scatter(X[:, 0], X[:, 1],\n",
    "                    s=40, \n",
    "                    c=np.array(list(cluster_dict_list[idx_array[i]].values())),\n",
    "                    marker=\"o\",\n",
    "                    cmap=cmap,\n",
    "                    norm=None,\n",
    "                    alpha=1,\n",
    "                    edgecolor=\"k\")\n",
    "        return scatter,\n",
    "\n",
    "\n",
    "\n",
    "    ani = FuncAnimation(fig, animate,interval=100,init_func=init,frames=frames,repeat=False, blit=True)\n",
    "\n",
    "    #ani.save('animation.gif')\n",
    "\n",
    "\n",
    "\n",
    "    #writer=animation.PillowWriter()\n",
    "\n",
    "    #writer = animation.FFMpegWriter(fps=2,bitrate=1000)\n",
    "\n",
    "    ani.save(outfile, dpi=200)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def pogo(dataset):\n",
    "    rips_complex = gudhi.RipsComplex(points=dataset,max_edge_length=100)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=1)\n",
    "    df = pd.DataFrame(simplex_tree.get_filtration(), columns=['simplex','distance'])\n",
    "    #move through list and assign clusters to conected components\n",
    "\n",
    "    point_dict={i:0 for i in range(simplex_tree.num_vertices())}\n",
    "    counter=0\n",
    "\n",
    "    cluster_dict_list = []\n",
    "    for i in range(simplex_tree.num_vertices(),len(df)):\n",
    "        #if both points are still in cluster 0, assign both to a new cluster\n",
    "        if point_dict[df.loc[i,'simplex'][0]] == 0 and point_dict[df.loc[i,'simplex'][1]] == 0:\n",
    "            counter += 1\n",
    "            point_dict[df.loc[i,'simplex'][0]] = counter\n",
    "            point_dict[df.loc[i,'simplex'][1]] = counter\n",
    "            #if one point is in cluster 0 and one is not, assign the one in cluster 0 to the existing cluster\n",
    "        elif point_dict[df.loc[i,'simplex'][0]] == 0 and point_dict[df.loc[i,'simplex'][1]] != 0:\n",
    "            point_dict[df.loc[i,'simplex'][0]] = point_dict[df.loc[i,'simplex'][1]]\n",
    "\n",
    "            #and vice versa\n",
    "        elif point_dict[df.loc[i,'simplex'][0]] != 0 and point_dict[df.loc[i,'simplex'][1]] == 0:\n",
    "            point_dict[df.loc[i,'simplex'][1]] = point_dict[df.loc[i,'simplex'][0]]\n",
    "\n",
    "            #if both points are not in cluster 0 and not in the same cluster, merge clusters to the lower number cluster\n",
    "        elif point_dict[df.loc[i,'simplex'][0]] != 0 and point_dict[df.loc[i,'simplex'][1]] != 0 and point_dict[df.loc[i,'simplex'][0]] != point_dict[df.loc[i,'simplex'][1]]:\n",
    "            larger_cluster_number = max(point_dict[df.loc[i,'simplex'][0]], point_dict[df.loc[i,'simplex'][1]])\n",
    "            smaller_cluster_number = min(point_dict[df.loc[i,'simplex'][0]], point_dict[df.loc[i,'simplex'][1]])\n",
    "            for key, value in point_dict.items():\n",
    "                if value == larger_cluster_number:\n",
    "                    point_dict[key] = smaller_cluster_number\n",
    "        cluster_dict_list.append(point_dict.copy())\n",
    "        #print(point_dict)\n",
    "        #print('step',i)\n",
    "\n",
    "    \n",
    "    cutoff_value = 0\n",
    "    for i in range(len(cluster_dict_list)):\n",
    "        if cutoff_value == 0:\n",
    "            if all(value != 0 for value in cluster_dict_list[i].values()):\n",
    "                cutoff_value = i\n",
    "                print('dataset becomes connected at simplex #',cutoff_value)\n",
    "       \n",
    "    df = df[:cutoff_value]\n",
    "    #cluster_dict_list =\n",
    "    \n",
    "    #find the gaps between birth/death pairs\n",
    "\n",
    "    df['gaps'] = df['distance'] - df['distance'].shift(1)\n",
    "    \n",
    "    #find normalized distance\n",
    "    scaler = MinMaxScaler()\n",
    "    df['normed_distance'] = scaler.fit_transform(df['distance'].values.reshape(-1, 1))\n",
    "\n",
    "    #Mulitply the gaps by their normed location in the filtration, weighting features more heavily if they're later\n",
    "    df['normed_gaps'] = df['gaps'] * df['normed_distance']\n",
    "\n",
    "    norm_sum = df['normed_gaps'].sum()\n",
    "    df['probability_vector_normed_gaps'] = df['normed_gaps'] / norm_sum\n",
    "    \n",
    "    gap_sum = df['gaps'].sum()\n",
    "    df['probability_vector_gaps'] = df['normed_gaps'] / gap_sum\n",
    "\n",
    "    partition = 10\n",
    "    candidates = [i for i in df.nlargest(partition ,['gaps']).index if i in df.nlargest( partition,['normed_gaps']).index]\n",
    "    #print('possible cuttoffs' , candidates)\n",
    "    idx = cutoff_value - 1\n",
    "    #idx = candidates[0]\n",
    "    print('idx' ,idx)\n",
    "\n",
    "    print('your dataset appears to have', len(np.unique(np.array(list(cluster_dict_list[idx].values())))),'clusters')\n",
    "\n",
    "    #print( np.array(list(cluster_dict_list[idx].values())))\n",
    "\n",
    "    print(simplex_tree.num_vertices() , 'vertices')  \n",
    "    print(simplex_tree.num_simplices(), 'simplices')\n",
    "    print('dimension:', simplex_tree.dimension())\n",
    "\n",
    "    size=10\n",
    "    plt.figure(figsize=(size,size))\n",
    "    plt.scatter(dataset[:, 0], dataset[:, 1],\n",
    "                s=30, \n",
    "                c=np.array(list(cluster_dict_list[idx].values())),\n",
    "                marker=\"o\",\n",
    "                cmap='rainbow',\n",
    "                norm=None,\n",
    "                alpha=.9,\n",
    "                edgecolor=\"k\")\n",
    "    \n",
    "    return cluster_dict_list, candidates, idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "cluster_dict_list, candidates, idx = pogo(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "true = y\n",
    "pred = np.array(list(cluster_dict_list[idx].values()))\n",
    "\n",
    "print('adjusted rand score =' , metrics.adjusted_rand_score(true, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
